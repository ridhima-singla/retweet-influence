{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import seed\n",
    "from random import randint\n",
    "from numpy import array\n",
    "from math import ceil\n",
    "from math import log10\n",
    "from math import sqrt\n",
    "from numpy import argmax\n",
    "from tensorflow.python.keras.models import Sequential \n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import LSTM\n",
    "from tensorflow.python.keras.layers import TimeDistributed\n",
    "from tensorflow.python.keras.layers import RepeatVector\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7505\n",
      "4\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# read file\n",
    "with open('bar_lstm.json', 'r') as myfile:\n",
    "    data=myfile.read()\n",
    "obj = json.loads(data)\n",
    "#craeting lists\n",
    "length_list1=[]\n",
    "length_list2=[]\n",
    "length_list3=[]\n",
    "length_list4=[]\n",
    "full_list=[]\n",
    "for i in range(0,len(obj)):\n",
    "    seq_list=[]\n",
    "    length_list1.append(len(obj[i]['sequence'][0]))\n",
    "    length_list2.append(len(obj[i]['sequence'][1]))\n",
    "    length_list3.append(len(obj[i]['sequence'][2]))\n",
    "    length_list4.append(len(obj[i]['sequence'][3]))\n",
    "    for j in range(0,4):\n",
    "        list_of_zeros=[0]*400\n",
    "        for k in range(len(obj[i]['sequence'][j])):\n",
    "            list_of_zeros[obj[i]['sequence'][j][k]]=1\n",
    "        seq_list.append(list_of_zeros)\n",
    "    full_list.append(seq_list)\n",
    "#size of full_list is 7505 X 4 X 400\n",
    "#finding size of full_list\n",
    "print(len(full_list))\n",
    "print(len(full_list[0]))\n",
    "print(len(full_list[0][0]))\n",
    "#print(len(full_list[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_column1=[]\n",
    "data_column2=[]\n",
    "data_column3=[]\n",
    "data_column4=[]\n",
    "for i in range(len(full_list)):\n",
    "    data_column1.append(full_list[i][0])\n",
    "    data_column2.append(full_list[i][1])\n",
    "    data_column3.append(full_list[i][2])\n",
    "    data_column4.append(full_list[i][3])\n",
    "\n",
    "    \n",
    "#creating 400 arrays for each length\n",
    "length_column1=[]\n",
    "length_column2=[]\n",
    "length_column3=[]\n",
    "length_column4=[]\n",
    "for i in range(len(length_list1)):\n",
    "    list_of_zeros=[0]*400\n",
    "    list_of_zeros[length_list1[i]]=1\n",
    "    length_column1.append(list_of_zeros)\n",
    "    \n",
    "for i in range(len(length_list2)):\n",
    "    list_of_zeros=[0]*400\n",
    "    list_of_zeros[length_list2[i]]=1\n",
    "    length_column2.append(list_of_zeros)\n",
    "    \n",
    "for i in range(len(length_list3)):\n",
    "    list_of_zeros=[0]*400\n",
    "    list_of_zeros[length_list3[i]]=1\n",
    "    length_column3.append(list_of_zeros)\n",
    "    \n",
    "for i in range(len(length_list4)):\n",
    "    list_of_zeros=[0]*400\n",
    "    list_of_zeros[length_list4[i]]=1\n",
    "    length_column4.append(list_of_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "data_column12=[]\n",
    "for i in range(len(data_column1)):\n",
    "    x=[]\n",
    "    x.append(data_column1[i])\n",
    "    x.append(data_column2[i])\n",
    "    data_column12.append(x)\n",
    "print(len(data_column12[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "data_column123=[]\n",
    "for i in range(len(data_column1)):\n",
    "    x=[]\n",
    "    x.append(data_column1[i])\n",
    "    x.append(data_column2[i])\n",
    "    x.append(data_column3[i])\n",
    "    data_column123.append(x)\n",
    "print(len(data_column123[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /newHomeDir/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 1, 400)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1, 400)            1281600   \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 1, 400)            160400    \n",
      "=================================================================\n",
      "Total params: 2,723,600\n",
      "Trainable params: 2,723,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model1_len=Sequential()\n",
    "model1_len.add(LSTM(400, input_shape=(1, 400)))\n",
    "model1_len.add(RepeatVector(1))\n",
    "model1_len.add(LSTM(400, return_sequences=True))\n",
    "model1_len.add(TimeDistributed(Dense(400, activation= 'softmax' )))\n",
    "model1_len.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "print(model1_len.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 1, 400)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 1, 400)            1281600   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 400)            160400    \n",
      "=================================================================\n",
      "Total params: 2,723,600\n",
      "Trainable params: 2,723,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#define LSTM for prediction of sequences of level1\n",
    "model1=Sequential()\n",
    "model1.add(LSTM(400, input_shape=(1, 400)))\n",
    "model1.add(RepeatVector(1))\n",
    "model1.add(LSTM(400, return_sequences=True))\n",
    "model1.add(TimeDistributed(Dense(400, activation= 'softmax' )))\n",
    "model1.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 1, 400)            0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 1, 400)            1281600   \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 1, 400)            160400    \n",
      "=================================================================\n",
      "Total params: 2,723,600\n",
      "Trainable params: 2,723,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#define LSTM for prediction of length of level2\n",
    "model2_len=Sequential()\n",
    "model2_len.add(LSTM(400, input_shape=(2, 400)))\n",
    "model2_len.add(RepeatVector(1))\n",
    "model2_len.add(LSTM(400, return_sequences=True))\n",
    "model2_len.add(TimeDistributed(Dense(400, activation= 'softmax' )))\n",
    "model2_len.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "print(model2_len.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 1, 400)            0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 1, 400)            1281600   \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 1, 400)            160400    \n",
      "=================================================================\n",
      "Total params: 2,723,600\n",
      "Trainable params: 2,723,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#define LSTM for prediction of sequences of level2\n",
    "model2=Sequential()\n",
    "model2.add(LSTM(400, input_shape=(2, 400)))\n",
    "model2.add(RepeatVector(1))\n",
    "model2.add(LSTM(400, return_sequences=True))\n",
    "model2.add(TimeDistributed(Dense(400, activation= 'softmax' )))\n",
    "model2.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 1, 400)            0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 1, 400)            1281600   \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 1, 400)            160400    \n",
      "=================================================================\n",
      "Total params: 2,723,600\n",
      "Trainable params: 2,723,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#define LSTM for prediction of length of level3\n",
    "model3_len=Sequential()\n",
    "model3_len.add(LSTM(400, input_shape=(3, 400)))\n",
    "model3_len.add(RepeatVector(1))\n",
    "model3_len.add(LSTM(400, return_sequences=True))\n",
    "model3_len.add(TimeDistributed(Dense(400, activation= 'softmax' )))\n",
    "model3_len.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "print(model3_len.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "repeat_vector_5 (RepeatVecto (None, 1, 400)            0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 1, 400)            1281600   \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 1, 400)            160400    \n",
      "=================================================================\n",
      "Total params: 2,723,600\n",
      "Trainable params: 2,723,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#define LSTM for prediction of sequences of level3\n",
    "model3=Sequential()\n",
    "model3.add(LSTM(400, input_shape=(3, 400)))\n",
    "model3.add(RepeatVector(1))\n",
    "model3.add(LSTM(400, return_sequences=True))\n",
    "model3.add(TimeDistributed(Dense(400, activation= 'softmax' )))\n",
    "model3.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping data for level 1\n",
    "#for length prediction\n",
    "X=np.array(data_column1)\n",
    "Y=np.array(length_column2)\n",
    "X_train1, X_test1, Y_train1_len, Y_test1_len = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "X_train1 = np.reshape(X_train1, (X_train1.shape[0], 1, X_train1.shape[1]))\n",
    "Y_train1_len = np.reshape(Y_train1_len, (Y_train1_len.shape[0], 1, Y_train1_len.shape[1]))\n",
    "\n",
    "#for sequence prediction\n",
    "X=np.array(data_column1)\n",
    "Y=np.array(data_column2)\n",
    "X_train1, X_test1, Y_train1, Y_test1 = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "X_train1 = np.reshape(X_train1, (X_train1.shape[0], 1, X_train1.shape[1]))\n",
    "Y_train1 = np.reshape(Y_train1, (Y_train1.shape[0], 1, Y_train1.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping data for level 2\n",
    "\n",
    "#for length prediction\n",
    "X=np.array(data_column12)\n",
    "Y=np.array(length_column3)\n",
    "X_train2, X_test2, Y_train2_len, Y_test2_len = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "Y_train2_len = np.reshape(Y_train2_len, (Y_train2_len.shape[0], 1, Y_train2_len.shape[1]))\n",
    "\n",
    "#for sequence prediction\n",
    "X=np.array(data_column12)\n",
    "Y=np.array(data_column3)\n",
    "X_train2, X_test2, Y_train2, Y_test2 = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "Y_train2 = np.reshape(Y_train2, (Y_train2.shape[0], 1, Y_train2.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5028, 3, 400)\n",
      "(5028, 3, 400)\n"
     ]
    }
   ],
   "source": [
    "#reshaping data for level 3\n",
    "\n",
    "#for length prediction\n",
    "X=np.array(data_column123)\n",
    "Y=np.array(length_column4)\n",
    "X_train3, X_test3, Y_train3_len, Y_test3_len = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "print(X_train3.shape)\n",
    "Y_train3_len = np.reshape(Y_train3_len, (Y_train3_len.shape[0], 1, Y_train3_len.shape[1]))\n",
    "\n",
    "#for sequence prediction\n",
    "X=np.array(data_column123)\n",
    "Y=np.array(data_column4)\n",
    "X_train3, X_test3, Y_train3, Y_test3 = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "Y_train3 = np.reshape(Y_train3, (Y_train3.shape[0], 1, Y_train3.shape[1]))\n",
    "print(X_train3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5028 samples\n",
      "Epoch 1/5\n",
      "5028/5028 [==============================] - 18s 4ms/sample - loss: 2.5949 - acc: 0.2753\n",
      "Epoch 2/5\n",
      "5028/5028 [==============================] - 12s 2ms/sample - loss: 1.1870 - acc: 0.5338\n",
      "Epoch 3/5\n",
      "5028/5028 [==============================] - 13s 3ms/sample - loss: 0.5256 - acc: 0.8540\n",
      "Epoch 4/5\n",
      "5028/5028 [==============================] - 13s 3ms/sample - loss: 0.3623 - acc: 0.8860\n",
      "Epoch 5/5\n",
      "5028/5028 [==============================] - 13s 3ms/sample - loss: 0.3032 - acc: 0.9041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe1ca5ea0f0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model fitting for length prediction level1\n",
    "model1_len.fit(X_train1,Y_train1_len, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5028 samples\n",
      "Epoch 1/3\n",
      "5028/5028 [==============================] - 18s 4ms/sample - loss: 14.0560 - acc: 0.4741\n",
      "Epoch 2/3\n",
      "5028/5028 [==============================] - 12s 2ms/sample - loss: 10.4372 - acc: 0.5638\n",
      "Epoch 3/3\n",
      "5028/5028 [==============================] - 12s 2ms/sample - loss: 6.7740 - acc: 0.5086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe18010dfd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model fitting for sequence prediction\n",
    "model1.fit(X_train1,Y_train1, epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5028 samples\n",
      "Epoch 1/5\n",
      "5028/5028 [==============================] - 19s 4ms/sample - loss: 1.8997 - acc: 0.4495\n",
      "Epoch 2/5\n",
      "5028/5028 [==============================] - 15s 3ms/sample - loss: 0.5413 - acc: 0.8117\n",
      "Epoch 3/5\n",
      "5028/5028 [==============================] - 16s 3ms/sample - loss: 0.3252 - acc: 0.8982\n",
      "Epoch 4/5\n",
      "5028/5028 [==============================] - 16s 3ms/sample - loss: 0.2484 - acc: 0.9298\n",
      "Epoch 5/5\n",
      "5028/5028 [==============================] - 16s 3ms/sample - loss: 0.2301 - acc: 0.9320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe17879a978>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_len.fit(X_train2,Y_train2_len, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5028 samples\n",
      "Epoch 1/3\n",
      "5028/5028 [==============================] - 19s 4ms/sample - loss: 6.8589 - acc: 0.2422\n",
      "Epoch 2/3\n",
      "5028/5028 [==============================] - 15s 3ms/sample - loss: 3.3237 - acc: 0.5642\n",
      "Epoch 3/3\n",
      "5028/5028 [==============================] - 16s 3ms/sample - loss: 2.4148 - acc: 0.6102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe13ae6fd68>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model fitting for sequence prediction\n",
    "model2.fit(X_train2,Y_train2, epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5028 samples\n",
      "Epoch 1/4\n",
      "5028/5028 [==============================] - 22s 4ms/sample - loss: 1.2264 - acc: 0.7631\n",
      "Epoch 2/4\n",
      "5028/5028 [==============================] - 18s 4ms/sample - loss: 0.3797 - acc: 0.8715\n",
      "Epoch 3/4\n",
      "5028/5028 [==============================] - 19s 4ms/sample - loss: 0.2353 - acc: 0.9286\n",
      "Epoch 4/4\n",
      "5028/5028 [==============================] - 19s 4ms/sample - loss: 0.1752 - acc: 0.9487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe1843e4fd0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3_len.fit(X_train3,Y_train3_len, epochs=4, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5028 samples\n",
      "Epoch 1/3\n",
      "5028/5028 [==============================] - 22s 4ms/sample - loss: 3.0585 - acc: 0.5416\n",
      "Epoch 2/3\n",
      "5028/5028 [==============================] - 18s 4ms/sample - loss: 0.9083 - acc: 0.8439\n",
      "Epoch 3/3\n",
      "5028/5028 [==============================] - 19s 4ms/sample - loss: 0.7021 - acc: 0.8516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe13ae74c88>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model fitting for sequence prediction\n",
    "model3.fit(X_train3,Y_train3, epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for level 1 reshaping of test data\n",
    "def test_data1(testing_data):\n",
    "    X=testing_data\n",
    "    X=np.array(X)\n",
    "    X= np.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
    "    result1_len=model1_len.predict(X)\n",
    "    length=result1_len[0][0].argmax(axis=0)\n",
    "    predict=model1.predict(X)\n",
    "    l=np.argpartition(predict[0][0], -length)[-length:]\n",
    "    l=np.sort(l)\n",
    "    return l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data2(testing_data):\n",
    "    X=testing_data\n",
    "    X=np.array(X)\n",
    "    result1_len=model2_len.predict(X)\n",
    "    length=result1_len[0][0].argmax(axis=0)\n",
    "    predict=model2.predict(X)\n",
    "    l=np.argpartition(predict[0][0], -length)[-length:]\n",
    "    l=np.sort(l)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data3(testing_data):\n",
    "    X=testing_data\n",
    "    X=np.array(X)\n",
    "    result1_len=model3_len.predict(X)\n",
    "    length=result1_len[0][0].argmax(axis=0)\n",
    "    predict=model3.predict(X)\n",
    "    l=np.argpartition(predict[0][0], -length)[-length:]\n",
    "    l=np.sort(l)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(data):\n",
    "    list_of_zeros=[0]*400   \n",
    "    for i in range(len(data)):\n",
    "        list_of_zeros[data[i]]=1 \n",
    "        \n",
    "    return (list_of_zeros)  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed Data               [48, 398, 3, 287]\n",
      "Predicted Level 1 data  [  2 186 369]\n",
      "Predicted Level 2 data  [208 341]\n",
      "Predicted Level 3 data  [221]\n"
     ]
    }
   ],
   "source": [
    "list_=[]\n",
    "data=[48, 398, 3, 287]\n",
    "print(\"Seed Data              \",end=\" \")\n",
    "print(data)\n",
    "start=data_preprocess(data)\n",
    "list_.append(start)\n",
    "level1=test_data1(list_)\n",
    "print(\"Predicted Level 1 data \",end=\" \")\n",
    "print(level1)\n",
    "#level 2\n",
    "start=data_preprocess(level1)\n",
    "list_.append(start)\n",
    "x=[]\n",
    "x.append(list_)\n",
    "level2=test_data2(x)\n",
    "print(\"Predicted Level 2 data \",end=\" \")\n",
    "print(level2)\n",
    "#level 3\n",
    "start=data_preprocess(level2)\n",
    "list_.append(start)\n",
    "x=[]\n",
    "x.append(list_)\n",
    "level3=test_data3(x)\n",
    "print(\"Predicted Level 3 data \",end=\" \")\n",
    "print(level3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
